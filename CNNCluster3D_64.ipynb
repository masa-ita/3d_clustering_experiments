{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2295e8e0",
   "metadata": {},
   "source": [
    "# Clustering Voxel data with k-means++ and 3D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ee0ec",
   "metadata": {},
   "source": [
    "## for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47a1d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a117f4c",
    "outputId": "db85c1ea-99f3-4271-baec-b05049c45edd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34606f3d",
   "metadata": {
    "id": "c9c2e582"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/drive/MyDrive/data/mn10_64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b101526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-determinism kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243806a3",
   "metadata": {
    "id": "b2afbb09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import random\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fe09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=200):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # optional\n",
    "    # for numpy.random\n",
    "    np.random.seed(seed)\n",
    "    # for built-in random\n",
    "    random.seed(seed)\n",
    "    # for hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    \n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df59e8",
   "metadata": {
    "id": "7eef2634"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_DIR = '3DCNNx4_64_GAP_pca'\n",
    "NUM_CLASSES = 10\n",
    "NUM_CLUSTERS = 10\n",
    "NUM_PCA_COMPONENTS = 10\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "CLUSTERING_INTERVAL = 5\n",
    "WHOLE_CYCLES = 20\n",
    "num_epochs = CLUSTERING_INTERVAL * WHOLE_CYCLES\n",
    "print('total epochs: ', num_epochs)\n",
    "\n",
    "DATA_DIR = '/content/mn10/64'\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b463c9",
   "metadata": {
    "id": "bf270b35"
   },
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = keras.Sequential([\n",
    "    layers.Conv3D(filters=64, kernel_size=3, padding='same', activation='relu',\n",
    "                  input_shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)),\n",
    "    layers.MaxPool3D(pool_size=2),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv3D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPool3D(pool_size=2),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv3D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPool3D(pool_size=2),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv3D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPool3D(pool_size=2),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.GlobalAveragePooling3D(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b46621",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Activation(keras.activations.relu),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f69315",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(feature_extractor,\n",
    "                         to_file=os.path.join(EXPERIMENT_DIR, 'feature_extractor.png'),\n",
    "                         show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977364c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier,\n",
    "                         to_file=os.path.join(EXPERIMENT_DIR, 'classifier.png'),\n",
    "                         show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f98500",
   "metadata": {
    "id": "a692b587"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=NUM_PCA_COMPONENTS)\n",
    "stdsc = StandardScaler()\n",
    "kmc = KMeans(n_clusters=NUM_CLUSTERS, init='k-means++', n_init=10, max_iter=300,\n",
    "                       tol=0.0001, verbose=0, random_state=123, copy_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbc7d5",
   "metadata": {
    "id": "81e74dd6"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67563d37",
   "metadata": {
    "id": "c2e4f85f"
   },
   "outputs": [],
   "source": [
    "categories = ['bathtub', 'bed', 'chair', 'desk', 'dresser',\n",
    "              'monitor', 'night_stand', 'sofa', 'table', 'toilet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d963afb",
   "metadata": {
    "id": "e9f92c4b"
   },
   "outputs": [],
   "source": [
    "train_pattern = DATA_DIR +'/train/*.npy'\n",
    "\n",
    "train_list_ds = tf.data.Dataset.list_files(train_pattern, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c738e",
   "metadata": {
    "id": "22d76cda"
   },
   "outputs": [],
   "source": [
    "cat_re = re.compile(r'.+/(.+?)_[0-9]+\\.npy')\n",
    "train_labels = [cat_re.match(item.numpy().decode())[1] for item in train_list_ds]\n",
    "train_ids = [categories.index(cat) for cat in train_labels]\n",
    "train_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_ids, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27b326",
   "metadata": {
    "id": "7898f33c"
   },
   "outputs": [],
   "source": [
    "def read_npy_file(path):\n",
    "    data = np.load(path.numpy())\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    return tf.convert_to_tensor(data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a36a3d",
   "metadata": {
    "id": "2046b72c"
   },
   "outputs": [],
   "source": [
    "train_3d_ds = train_list_ds.map(\n",
    "        lambda item: tf.py_function(read_npy_file, [item], tf.float32)).cache(filename='./cache.tf-data')\n",
    "train_dataset = tf.data.Dataset.zip((train_3d_ds, train_label_ds)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627b52f",
   "metadata": {
    "id": "cda1de99"
   },
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93383f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = os.path.join(EXPERIMENT_DIR, 'logs', current_time , 'train')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f8615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "757969d3",
    "outputId": "4ae24209-a8f6-4487-a34d-896095a1efa9"
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1.0e-4)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with train_summary_writer.as_default():\n",
    "        if epoch % CLUSTERING_INTERVAL == 0:\n",
    "            features_list = [feature_extractor(batch, training=False) for batch in train_3d_ds.batch(BATCH_SIZE)]\n",
    "            features = np.vstack(features_list)\n",
    "            features_pca = pca.fit_transform(features)\n",
    "            features_std = stdsc.fit_transform(features_pca)\n",
    "            km_predictions = kmc.fit_predict(features_std)\n",
    "            cluster_matrix = np.zeros((NUM_CLASSES, NUM_CLUSTERS), dtype=np.int32)\n",
    "            for i, cat_id in enumerate(train_ids):\n",
    "                cluster_matrix[cat_id, km_predictions[i]] += 1\n",
    "            print(\"Epoch: {}, Distortion: {:.2f}\".format(epoch, kmc.inertia_))\n",
    "            tf.summary.scalar('sse', kmc.inertia_, step=epoch)\n",
    "            print(cluster_matrix)\n",
    "            pseudo_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(km_predictions, tf.int64))\n",
    "            train_dataset = tf.data.Dataset.zip((train_3d_ds, pseudo_label_ds)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                y = classifier(x_batch_train, training=True)\n",
    "\n",
    "                loss = loss_fn(y_batch_train, y)\n",
    "\n",
    "            grads = tape.gradient(loss, classifier.trainable_weights)\n",
    "\n",
    "            optimizer.apply_gradients(zip(grads, classifier.trainable_weights))\n",
    "\n",
    "            train_loss(loss)\n",
    "            train_accuracy(y_batch_train, y)\n",
    "\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "        print (template.format(epoch+1,\n",
    "                             train_loss.result(),\n",
    "                             train_accuracy.result()*100))\n",
    "\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36462b49",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.save(os.path.join(EXPERIMENT_DIR, 'saved_models', 'feature_extractor'))\n",
    "classifier.save(os.path.join(EXPERIMENT_DIR, 'saved_models', 'classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9492f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNNCluster3D_pca_64.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
