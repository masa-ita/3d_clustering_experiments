{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "MetricLearning3DWithVolumentation-3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUuPSqnCmrwh"
      },
      "source": [
        "# Metric Learning 3D-CNN for clustering"
      ],
      "id": "YUuPSqnCmrwh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MEYWGlmxx1"
      },
      "source": [
        "## for Google Colab"
      ],
      "id": "Z0MEYWGlmxx1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a117f4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "9a117f4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9c2e582"
      },
      "source": [
        "!unzip -q /content/drive/MyDrive/data/mn10_64.zip"
      ],
      "id": "c9c2e582",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShzHa4E6unZu"
      },
      "source": [
        "!pip install volumentations-3D tensorflow-addons tensorflow-determinism kaleido"
      ],
      "id": "ShzHa4E6unZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW_eeZRxm1Fp"
      },
      "source": [
        "## Import modules and set parameters"
      ],
      "id": "IW_eeZRxm1Fp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2afbb09"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from volumentations import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "id": "b2afbb09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A47FOhxTnkLx"
      },
      "source": [
        "def set_seed(seed=200):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # optional\n",
        "    # for numpy.random\n",
        "    np.random.seed(seed)\n",
        "    # for built-in random\n",
        "    random.seed(seed)\n",
        "    # for hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    \n",
        "set_seed(123)"
      ],
      "id": "A47FOhxTnkLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eef2634"
      },
      "source": [
        "EXPERIMENT_DIR = 'MetricLearning_64'\n",
        "\n",
        "NUM_SAMPLES = 10\n",
        "K = 4\n",
        "BATCH_SIZE = NUM_SAMPLES * K\n",
        "EPOCHS = 10\n",
        "\n",
        "DATA_DIR = '/content/mn10/64'\n",
        "IMAGE_SIZE = 64\n",
        "NUM_CHANNELS = 1"
      ],
      "id": "7eef2634",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d16dc394"
      },
      "source": [
        "os.makedirs(EXPERIMENT_DIR, exist_ok=True)"
      ],
      "id": "d16dc394",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf270b35"
      },
      "source": [
        "## Define models"
      ],
      "id": "bf270b35"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv8WgZDDZ3NP"
      },
      "source": [
        "class L2Constraint(keras.layers.Layer):\n",
        "    def __init__(self, alpha=16):\n",
        "        super(L2Constraint, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.alpha * tf.nn.l2_normalize(x)\n",
        "        return x"
      ],
      "id": "Lv8WgZDDZ3NP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8d48419"
      },
      "source": [
        "input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "x = layers.Conv3D(filters=64, kernel_size=3, activation='relu')(input)\n",
        "x = layers.MaxPool3D(pool_size=2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv3D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "x = layers.MaxPool3D(pool_size=2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv3D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "x = layers.MaxPool3D(pool_size=2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv3D(filters=256, kernel_size=3, activation='relu')(x)\n",
        "x = layers.MaxPool3D(pool_size=2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.GlobalAveragePooling3D()(x)\n",
        "features = layers.Dense(128)(x)\n",
        "output = tf.nn.l2_normalize(x, axis=-1)\n",
        "\n",
        "model = keras.Model(inputs=[input], outputs=[output])\n",
        "feature_extractor = keras.Model(inputs=[input], outputs=[features])"
      ],
      "id": "a8d48419",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfe4e7b6"
      },
      "source": [
        "model.summary()"
      ],
      "id": "bfe4e7b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81e74dd6"
      },
      "source": [
        "## Prepare data"
      ],
      "id": "81e74dd6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2e4f85f"
      },
      "source": [
        "categories = ['bathtub', 'bed', 'chair', 'desk', 'dresser',\n",
        "              'monitor', 'night_stand', 'sofa', 'table', 'toilet']"
      ],
      "id": "c2e4f85f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9f92c4b"
      },
      "source": [
        "train_pattern = DATA_DIR +'/train/*.npy'\n",
        "\n",
        "train_list = glob(train_pattern)"
      ],
      "id": "e9f92c4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22d76cda"
      },
      "source": [
        "cat_re = re.compile(r'.+/(.+?)_[0-9]+\\.npy')\n",
        "train_labels = [cat_re.match(item)[1] for item in train_list]\n",
        "train_ids = [categories.index(cat) for cat in train_labels]"
      ],
      "id": "22d76cda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7898f33c"
      },
      "source": [
        "def get_augmentation(patch_size):\n",
        "    return Compose([\n",
        "        Rotate((-15, 15), (-15, 15), (-15, 15), interpolation=1, p=0.5),\n",
        "        ElasticTransform((0, 0.25), interpolation=1, p=0.1),\n",
        "        Flip(0, p=0.5),\n",
        "        Flip(1, p=0.5),\n",
        "        Flip(2, p=0.5),\n",
        "        RandomRotate90((0, 1), p=0.5),\n",
        "    ], p=1.0)\n",
        "\n",
        "aug = get_augmentation((IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE))"
      ],
      "id": "7898f33c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjzCg498DL5"
      },
      "source": [
        "class OrderedStream(keras.utils.Sequence):\n",
        "    def __init__(self, file_list, ids_list, batch_size=32):\n",
        "        self.file_list = file_list\n",
        "        self.ids_list = ids_list\n",
        "        self.batch_size = batch_size\n",
        "        self.num_files = len(file_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.num_files / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = []\n",
        "        from_idx = idx * self.batch_size\n",
        "        to_idx = min((idx + 1) * self.batch_size, self.num_files)\n",
        "        for file_path in self.file_list[from_idx : to_idx]:\n",
        "            x.append(self._read_npy_file(file_path))\n",
        "        y = self.ids_list[from_idx : to_idx]\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _read_npy_file(self, path):\n",
        "        data = np.load(path)\n",
        "        data = np.expand_dims(data, axis=-1)\n",
        "        return data.astype(np.float32)"
      ],
      "id": "iEjzCg498DL5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJKI68MAGpBC"
      },
      "source": [
        "ordered_stream = OrderedStream(train_list, train_ids, batch_size=BATCH_SIZE)"
      ],
      "id": "jJKI68MAGpBC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIaEDsBhwbeh"
      },
      "source": [
        "class AugmentedStream(keras.utils.Sequence):\n",
        "    def __init__(self, file_list, aug, num_samples=4, k=4):\n",
        "        self.file_list = file_list\n",
        "        self.num_files = len(file_list)\n",
        "        self.file_indices = np.random.permutation(np.arange(self.num_files))\n",
        "        self.aug = aug\n",
        "        self.num_samples = num_samples\n",
        "        self.k = k\n",
        "        self.num_batchs = self.num_files // self.num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.num_files / float(self.num_samples)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = [], []\n",
        "        from_idx = idx * self.num_samples\n",
        "        to_idx = min((idx + 1) * self.num_samples, self.num_files)\n",
        "        sample_indices = self.file_indices[from_idx : to_idx]\n",
        "        for idx in sample_indices:\n",
        "            data = {'image': self._read_npy_file(self.file_list[idx])}\n",
        "            for i in range(self.k):\n",
        "                aug_data = self.aug(**data)\n",
        "                x.append(aug_data['image'])\n",
        "                y.append(idx)\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _read_npy_file(self, path):\n",
        "        data = np.load(path)\n",
        "        data = np.expand_dims(data, axis=-1)\n",
        "        return data.astype(np.float32)"
      ],
      "id": "YIaEDsBhwbeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsooR5n802ek"
      },
      "source": [
        "aug_stream = AugmentedStream(train_list, aug, num_samples=NUM_SAMPLES, k=K)"
      ],
      "id": "LsooR5n802ek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cda1de99"
      },
      "source": [
        "## Train model"
      ],
      "id": "cda1de99"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "757969d3"
      },
      "source": [
        "loss_fn = tfa.losses.TripletSemiHardLoss(margin=0.1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1.0e-4)\n",
        "\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "\n",
        "model.fit(aug_stream, epochs=EPOCHS)"
      ],
      "id": "757969d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeMdaS8pen0"
      },
      "source": [
        "model.save(os.path.join(EXPERIMENT_DIR, 'saved_models', 'whole_model'))\n",
        "feature_extractor.save(os.path.join(EXPERIMENT_DIR, 'saved_models', 'feature_extractor'))"
      ],
      "id": "FYeMdaS8pen0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkMo941A_5QI"
      },
      "source": [
        "## Copy experiment directory to Google Drive"
      ],
      "id": "KkMo941A_5QI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Z05B5SWVcg"
      },
      "source": [
        "!cp -r $EXPERIMENT_DIR /content/drive/MyDrive/"
      ],
      "id": "_1Z05B5SWVcg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTV89t8gz5D1"
      },
      "source": [
        ""
      ],
      "id": "aTV89t8gz5D1",
      "execution_count": null,
      "outputs": []
    }
  ]
}